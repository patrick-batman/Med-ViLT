{"cells":[{"cell_type":"markdown","metadata":{"id":"VXSFwOu8XDAJ"},"source":["# Setting Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEaVq2c_W4-k"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import copy\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-79anAWIXBlJ"},"outputs":[],"source":["df = pd.read_csv('data.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"dcOmsTByXGAB"},"source":["# Positive Sample Generation"]},{"cell_type":"markdown","metadata":{"id":"VMFVY415rOd8"},"source":["### 1. Synonym Replacement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEw2VVWaceyw"},"outputs":[],"source":["!pip install requests nlpaug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7rXQu6U_5lp"},"outputs":[],"source":["import nlpaug\n","import nlpaug.augmenter.word as naw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsKfNT_p_9tK"},"outputs":[],"source":["aug = naw.SynonymAug(aug_src='wordnet',aug_max=2)\n","synonyms = copy.deepcopy(df)\n","synonyms['premise'] = synonyms['premise'].map(lambda x:aug.augment(x,n=1)[0])\n","synonyms.head()"]},{"cell_type":"markdown","metadata":{"id":"dkrawJu5CWko"},"source":["### Random Deletion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLh0eIQ1d7I0"},"outputs":[],"source":["def random_deletion(words, p):\n","\n","    words = words.split()\n","\n","    #obviously, if there's only one word, don't delete it\n","    if len(words) == 1:\n","        return words\n","\n","    #randomly delete words with probability p\n","    new_words = []\n","    for word in words:\n","        r = random.uniform(0, 1)\n","        if r > p:\n","            new_words.append(word)\n","\n","    #if you end up deleting all words, just return a random word\n","    if len(new_words) == 0:\n","        rand_int = random.randint(0, len(words)-1)\n","        return [words[rand_int]]\n","\n","    sentence = ' '.join(new_words)\n","\n","    return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTT20oORd8Rh"},"outputs":[],"source":["del_rand = copy.deepcopy(df)\n","del_rand['premise'] = del_rand['premise'].map(lambda x: random_deletion(x, 0.2))\n","del_rand.head()"]},{"cell_type":"markdown","metadata":{"id":"K9ERImLJCghc"},"source":["### Random Swap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZQIHjwmJgu6n"},"outputs":[],"source":["def swap_word(new_words):\n","\n","    random_idx_1 = random.randint(0, len(new_words)-1)\n","    random_idx_2 = random_idx_1\n","    counter = 0\n","\n","    while random_idx_2 == random_idx_1:\n","        random_idx_2 = random.randint(0, len(new_words)-1)\n","        counter += 1\n","\n","        if counter > 3:\n","            return new_words\n","\n","    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n","    return new_words\n","\n","def random_swap(words, n):\n","\n","    words = words.split()\n","    new_words = words.copy()\n","    # n is the number of words to be swapped\n","    for _ in range(n):\n","        new_words = swap_word(new_words)\n","\n","    sentence = ' '.join(new_words)\n","\n","    return sentence\n","\n","swap_rand = copy.deepcopy(df)\n","swap_rand['premise'] = swap_rand['premise'].map(lambda x: random_swap(x, 1))\n","swap_rand.head()"]},{"cell_type":"markdown","metadata":{"id":"8Xaq3NuZCjaj"},"source":["### Random Insertion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bhvtCPunUDb"},"outputs":[],"source":["from nltk.corpus import wordnet\n","\n","def get_synonyms(word):\n","\n","    synonyms = set()\n","\n","    for syn in wordnet.synsets(word):\n","        for l in syn.lemmas():\n","            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n","            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n","            synonyms.add(synonym)\n","    if word in synonyms:\n","        synonyms.remove(word)\n","\n","    return list(synonyms)\n","\n","def random_insertion(words, n):\n","\n","    words = words.split()\n","    new_words = words.copy()\n","\n","    for _ in range(n):\n","        add_word(new_words)\n","\n","    sentence = ' '.join(new_words)\n","    return sentence\n","\n","def add_word(new_words):\n","\n","    synonyms = []\n","    counter = 0\n","\n","    while len(synonyms) < 1:\n","        random_word = new_words[random.randint(0, len(new_words)-1)]\n","        synonyms = get_synonyms(random_word)\n","        counter += 1\n","        if counter >= 10:\n","            return\n","\n","    random_synonym = synonyms[0]\n","    random_idx = random.randint(0, len(new_words)-1)\n","    new_words.insert(random_idx, random_synonym)\n","\n","\n","\n","ins_rand = copy.deepcopy(df)\n","ins_rand['premise'] = ins_rand['premise'].map(lambda x: random_insertion(x, 1))\n","ins_rand.head()"]},{"cell_type":"markdown","metadata":{"id":"F06eOkUqXZC4"},"source":["### Backtranslation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FTt4AWJXmic"},"outputs":[],"source":["!pip install transformers\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TXc6YvE7XIBD"},"outputs":[],"source":["from transformers import MarianMTModel, MarianTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4STCNfTZxuH"},"outputs":[],"source":["first_model_name = 'Helsinki-NLP/opus-mt-en-fr'\n","first_model_tkn = MarianTokenizer.from_pretrained(first_model_name)\n","first_model = MarianMTModel.from_pretrained(first_model_name)\n","\n","second_model_name = 'Helsinki-NLP/opus-mt-fr-en'\n","second_model_tkn = MarianTokenizer.from_pretrained(second_model_name)\n","second_model = MarianMTModel.from_pretrained(second_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4NTImyXaVp7"},"outputs":[],"source":["original_texts = [\"This article aims to perform the back translation for text data augmentation\",\n","          \"It is the 25th article by Zoumana on Medium. He loves to give back to the community\",\n","          \"The first model translates from English to French, which is a temporary process\",\n","          \"The second model finally translates back all the temporary french text into English\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3ZlzzxiaIY4"},"outputs":[],"source":["def format_batch_texts(language_code, batch_texts):\n","\n","  formated_bach = [\">>{}<< {}\".format(language_code, text) for text in batch_texts]\n","\n","  return formated_bach\n","\n","def perform_translation(batch_texts, model, tokenizer, language=\"fr\"):\n","    # Prepare the text data into appropriate format for the model\n","    formated_batch_texts = format_batch_texts(language, batch_texts)\n","\n","    # Generate translation using model\n","    translated = model.generate(**tokenizer(formated_batch_texts, return_tensors=\"pt\", padding=True))\n","\n","    # Convert the generated tokens indices back into text\n","    translated_texts = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n","\n","    return translated_texts\n","\n","def backkaro(x):\n","    translated_texts = perform_translation([x], first_model, first_model_tkn)\n","    back_translated_texts = perform_translation(translated_texts, second_model, second_model_tkn)\n","    return back_translated_texts[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cgk7_IKhbQoj"},"outputs":[],"source":["backtr = copy.deepcopy(df)\n","backtr['premise'] = backtr['premise'].map(lambda x: backkaro(x))\n","backtr.head()"]},{"cell_type":"markdown","metadata":{"id":"LF_STzFbbx-t"},"source":[]},{"cell_type":"markdown","metadata":{"id":"asxWnII2rOeI"},"source":["### Text augmentation using pretrained Masked Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVINUyIhlYi3"},"outputs":[],"source":["!pip install textattack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vxES5wIlcYi"},"outputs":[],"source":["import textattack"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyrArH3KrOeI"},"outputs":[],"source":["from textattack.augmentation import CLAREAugmenter\n","clare_aug = CLAREAugmenter()\n","textaug = copy.deepcopy(df)\n","textaug['premise'] = textaug['premise'].map(lambda x: clare_aug.augment(x))"]},{"cell_type":"markdown","metadata":{"id":"HJ14oQjmmHky"},"source":["## Combining all the approaches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMUsDiSmmLq8"},"outputs":[],"source":["positive_samples = pd.concat([synonyms, del_rand, swap_rand, ins_rand, backtr], ignore_index=True)\n","positive_samples"]},{"cell_type":"markdown","metadata":{"id":"oXEiaF1FXIcj"},"source":["# Negative Sample Generation"]},{"cell_type":"markdown","metadata":{"id":"AAuZMTg2rOeD"},"source":["### Slide and concatenate approach"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5mMEy8krOeE"},"outputs":[],"source":["conc = copy.deepcopy(positive_samples)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21fPTQQ8rOeE"},"outputs":[],"source":["conc['premise_tr'] = conc['premise'].shift(1)\n","conc['premise_tr'].loc[0] = conc['premise'].loc[1]\n","conc.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpI4CTh0rOeF"},"outputs":[],"source":["slide = pd.DataFrame(columns=['premise', 'hypothesis', 'label'])\n","slide[['premise', 'hypothesis', 'label']] = conc[['premise_tr','hypothesis','label']]\n","slide['label'] = 0\n","slide.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXyWT5cro1QD"},"outputs":[],"source":["slide.shape"]},{"cell_type":"markdown","metadata":{"id":"wE9CkU5kEM_h"},"source":["### Antonym Replacement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"th84JlivEMWT"},"outputs":[],"source":["aug = naw.AntonymAug(name='Antonym_Aug', aug_min=1, aug_max=10, aug_p=0.3, lang='eng', stopwords=None, tokenizer=None,\n","                     reverse_tokenizer=None, stopwords_regex=None, verbose=0)\n","\n","test_sentence_aug = aug.augment(\"very beautiful\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGYNgmP9EUl_"},"outputs":[],"source":["antonyms = copy.deepcopy(df)\n","antonyms['premise'] = antonyms['premise'].map(lambda x:aug.augment(x)[0])\n","antonyms['label'] = 0\n","antonyms.head()"]},{"cell_type":"markdown","metadata":{"id":"jPedWb0ZoeG4"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsbeGHzyoclP"},"outputs":[],"source":["final_dataset = pd.concat([positive_samples, slide, antonyms], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzIIojKSqCyD"},"outputs":[],"source":["final_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isqHg-pCkhOI"},"outputs":[],"source":["x = final_dataset[\"premise\"].astype(str) + \"[sep]\" +  final_dataset[\"hypothesis\"].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAurWeQOqLp7"},"outputs":[],"source":["!pip install -q sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qk4GCt5qdr_"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n","train_statements_embeddings = model.encode(list(x))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-AgW4Poq86g"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(train_statements_embeddings, final_dataset['label'], test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"1MO1J-4cw8Ne"},"source":["Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1w2H_8ouxFm"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","l_model = LogisticRegression(solver='liblinear', random_state=0)\n","l_model.fit(x_train, y_train)\n","l_model.score(x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"1DXeuTYnw-I7"},"source":["F1 Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EWDVa94vvTI-"},"outputs":[],"source":["y_preds = l_model.predict(x_test)\n","from sklearn.metrics import f1_score\n","print(f1_score(y_test,y_preds))"]},{"cell_type":"markdown","metadata":{"id":"xmu0qxXdxAYi"},"source":["Recall Score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iLw2TWPxA86"},"outputs":[],"source":["from sklearn.metrics import recall_score\n","print(recall_score(y_test,y_preds))"]}],"metadata":{"colab":{"collapsed_sections":["VXSFwOu8XDAJ","VMFVY415rOd8","dkrawJu5CWko","8Xaq3NuZCjaj","AAuZMTg2rOeD"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}