{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "NWzwqnQgetz8",
        "outputId": "bab154c8-4515-45de-8432-0da6b5c061ce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import ViltConfig, ViltProcessor\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from transformers import ViltForQuestionAnswering\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.utils.data import DataLoader\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_data=pd.read_csv(os.path.join(os.getcwd(),'test-data.csv'))\n",
        "epochs = 101\n",
        "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recieved one_hot_vectors\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from torch.nn.functional import one_hot\n",
        "# read the encodings.json file\n",
        "with open(os.path.join(os.getcwd(),'tokens.json'), 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# extract the unique_answers, answer_to_index, and predicted_answers lists\n",
        "unique_answers = data['unique_answers']\n",
        "answer_to_index = data['answer_to_index']\n",
        "predicted_answers = data['predicted_answers']\n",
        "\n",
        "print(\"recieved one_hot_vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1421"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(unique_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1421"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(answer_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kJ3sm7qGAyab"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, processor):\n",
        "    final_data = pd.read_csv('test-data.csv')\n",
        "    self.questions = final_data['question']\n",
        "    self.image_id = final_data['image']\n",
        "    self.processor = processor\n",
        "    self.answers = final_data['ans-hyp']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_id)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    # get image + text\n",
        "    questions = self.questions[idx]\n",
        "    img_path = os.path.join(os.getcwd(),'osfstorage-archive','VQA_RADImage',self.image_id[idx])\n",
        "    image = Image.open(img_path)\n",
        "    answer = self.answers[idx]\n",
        "    \n",
        "    encoding = self.processor(image, questions, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # remove batch dimension\n",
        "    for k,v in encoding.items():\n",
        "      encoding[k] = v.squeeze()\n",
        "\n",
        "    answer_words = answer.split('-')\n",
        "    scores = list()\n",
        "    if len(answer_words) > 1:\n",
        "      scores = [(1/len(answer_words)) for _ in range(len(answer_words))]\n",
        "    else: \n",
        "      scores = [1.0]\n",
        "    # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\n",
        "    targets = torch.zeros(len(answer_to_index))\n",
        "    print(len(answer_to_index))\n",
        "    scores_final = torch.tensor(scores)\n",
        "    for ans, score in zip(answer_words,scores_final):\n",
        "      targets[answer_to_index[ans]] = score\n",
        "    encoding[\"labels\"] = targets\n",
        "    return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "853dOjCJg_RW"
      },
      "outputs": [],
      "source": [
        "dataset = VQADataset(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKFd8ozdIY_q",
        "outputId": "aa029336-6f22-455b-a2bb-9f691cced2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1421\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FBcmLQ6AJb4M",
        "outputId": "20de3964-4d32-4c61-e6d0-f4cc2bf9ccae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1421\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'[CLS] where is the mass located? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.decode(dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AGxYDVSve-O6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1421\n"
          ]
        }
      ],
      "source": [
        "labels = torch.nonzero(dataset[0]['labels']).squeeze().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[474, 485]\n"
          ]
        }
      ],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__LNlpeffBh-",
        "outputId": "0ebcbde7-835e-49dd-af38-2a13414bf449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['plexus', 'choroid']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[unique_answers[label] for label in labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tThbn-ilfmYi"
      },
      "source": [
        "## Define model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sjOH0jYcfkAk",
        "outputId": "1204b379-7854-4653-d585-e0e8866ef73a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.1.weight', 'classifier.0.bias', 'classifier.0.weight', 'classifier.3.bias', 'classifier.1.bias', 'classifier.3.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViltForQuestionAnswering\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViltForQuestionAnswering(\n",
              "  (vilt): ViltModel(\n",
              "    (embeddings): ViltEmbeddings(\n",
              "      (text_embeddings): TextEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768)\n",
              "        (position_embeddings): Embedding(40, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (patch_embeddings): ViltPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      )\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViltEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViltLayer(\n",
              "          (attention): ViltAttention(\n",
              "            (attention): ViltSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViltSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViltIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViltOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViltPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
              "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Linear(in_features=1536, out_features=1421, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.classifier[3] = torch.nn.Linear(in_features=1536 , out_features=1421, bias=True)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8KRHccEw5DE"
      },
      "source": [
        "Next, we create a corresponding PyTorch DataLoader, which allows us to iterate over the dataset in batches.\n",
        "\n",
        "Due to the fact that the processor resizes images to not necessarily the same size, we leverage the `pad_and_create_pixel_mask` method of the processor to pad the pixel values of a batch and create a corresponding pixel mask, which is a tensor of shape (batch_size, height, width) indicating which pixels are real (1) and which are padding (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5anZ-ZyMEY"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's verify whether the model has actually learned something:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViltForQuestionAnswering(\n",
              "  (vilt): ViltModel(\n",
              "    (embeddings): ViltEmbeddings(\n",
              "      (text_embeddings): TextEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768)\n",
              "        (position_embeddings): Embedding(40, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (patch_embeddings): ViltPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
              "      )\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViltEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViltLayer(\n",
              "          (attention): ViltAttention(\n",
              "            (attention): ViltSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViltSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViltIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViltOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViltPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
              "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Linear(in_features=1536, out_features=1421, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load('/Users/raunakpandey/Downloads/ViLT_final_check_point_epoch95.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model = model.eval()\n",
        "# # - or -\n",
        "# model.train()\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG0UzoCfgpEF",
        "outputId": "015e48be-9f14-442d-fa44-3c8795bd357d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1421\n",
            "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'])\n"
          ]
        }
      ],
      "source": [
        "example = dataset[0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IDIsLxBMAdRI",
        "outputId": "a4fbe7ef-227a-4b57-ad8e-8ebd4dc122e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] where is the mass located? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processor.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we need to apply a sigmoid activation on the logits since the model is trained using binary cross-entropy loss (as it frames VQA as a multi-label classification task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add batch dimension + move to GPU\n",
        "example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}\n",
        "\n",
        "# forward pass\n",
        "outputs = model(**example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.45213961601257324 ventricles\n",
            "0.35632842779159546 lateral\n",
            "0.004177028313279152 and\n",
            "0.0010874273721128702 cerebrum\n",
            "0.0009801362175494432 anterior\n"
          ]
        }
      ],
      "source": [
        "logits = outputs.logits\n",
        "predicted_classes = torch.sigmoid(logits)\n",
        "\n",
        "probs, classes = torch.topk(predicted_classes, 5)\n",
        "\n",
        "for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n",
        "  print(prob, unique_answers[class_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open Ended Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "image = Image.open('/Users/raunakpandey/Documents/programming/projects/med-flamingo/ViLT/osfstorage-archive/VQA_RADImage/synpic16174.jpg')\n",
        "text = \"is this a male body?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9992679953575134 yes\n",
            "0.0017395588802173734 no\n",
            "2.0657535060308874e-05 maybe\n",
            "1.9025181245524436e-05 midline\n",
            "9.883326129056513e-06 right\n"
          ]
        }
      ],
      "source": [
        "# prepare inputs\n",
        "encoding = processor(image, text, return_tensors=\"pt\")\n",
        "encoding.to(device)\n",
        "\n",
        "# forward pass\n",
        "outputs = model(**encoding)\n",
        "logits = outputs.logits\n",
        "\n",
        "predicted_classes = torch.sigmoid(logits)\n",
        "\n",
        "probs, classes = torch.topk(predicted_classes, 5)\n",
        "\n",
        "for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n",
        "  print(prob, unique_answers[class_idx])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uN8Grgg0gK2x"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
